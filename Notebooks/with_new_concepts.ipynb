{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataset with added concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The added concepts are:\n",
    "\n",
    "- Intersection Points: Some digits have points where lines intersect (like '4', '8', '9',\n",
    "'0'), while others do not ('1', '2', '3', '5', '7'). This can be an interesting feature,\n",
    "labeled as \"HasIntersection\" or \"NoIntersection\".\n",
    "\n",
    "\n",
    "- Closed Loops: Certain digits contain closed loops (e.g., '6', '8', '9', '0'), while others\n",
    "do not. This could be a feature, labeled as \"ClosedLoop:present\" or\n",
    "\"ClosedLoop:absent\".\n",
    "\n",
    "\n",
    "- Presence of Horizontal/Vertical Lines: In addition to straight and curved lines,\n",
    "identifying the presence of specifically horizontal or vertical lines could be useful.\n",
    "For instance, '1', '4', '7' often have vertical lines, while '2', '5' have horizontal\n",
    "components.\n",
    "\n",
    "\n",
    "- Top/Bottom Heavy: Some digits have more weight at the top (like '9') or bottom\n",
    "(like '6'). This could be an interesting feature to capture, labeled as \"TopHeavy\" or\n",
    "\"BottomHeavy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimetypes import init\n",
    "from pyexpat import model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pdb\n",
    "import argparse\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class MNISTDatasetWithConcepts(Dataset):\n",
    "\tdef __init__(self,split,num_classes,transform):\n",
    "\t\tisTrain = False\n",
    "\t\tif split == \"train\":\n",
    "\t\t\tisTrain=True\n",
    "\t\tself.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\t\tself.data = MNIST(root = \"./synthetic_datasets\",train=isTrain, download=True)\n",
    "\t\tself.num_classes = num_classes\n",
    "\t\tself.transform = transform\n",
    "\t\t# print(len(set([self.data[i][1] for i in range(len(self.data))])))\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\tdef __getitem__(self,idx):\n",
    "\t\timg,label = self.data[idx][0],self.data[idx][1]\n",
    "\t\tonehot = torch.zeros((self.num_classes,))\n",
    "\t\tonehot[label] =1\n",
    "\t\tconcept = self.make_concepts_mnist(label)\n",
    "\t\tlabel = onehot.to(self.device)\n",
    "\t\treturn [self.transform(img).to(self.device),label,concept]\n",
    "\n",
    "\tdef make_concepts_mnist(self,label):\n",
    "\t\tif label == 0:\n",
    "\t\t\thard_label = torch.tensor([1,0,0,0,0,0,0,0,0,0]+[1,0,0,1,0,0,0,0])\n",
    "\t\telif label == 1:\n",
    "\t\t\thard_label = torch.tensor([0,1,0,0,0,0,0,0,0,0]+[0,1,0,0,1,1,0,1])\n",
    "\t\telif label == 2:\n",
    "\t\t\thard_label = torch.tensor([0,0,1,0,0,0,0,0,0,0]+[1,1,0,0,1,0,0,1])\n",
    "\t\telif label == 3:\n",
    "\t\t\thard_label = torch.tensor([0,0,0,1,0,0,0,0,0,0]+[1,0,0,0,0,0,0,0])\n",
    "\t\telif label == 4:\n",
    "\t\t\thard_label = torch.tensor([0,0,0,0,1,0,0,0,0,0]+[0,1,1,0,1,1,1,0])\n",
    "\t\telif label == 5:\n",
    "\t\t\thard_label = torch.tensor([0,0,0,0,0,1,0,0,0,0]+[1,1,0,0,1,1,1,0])\n",
    "\t\telif label == 6:\n",
    "\t\t\thard_label = torch.tensor([0,0,0,0,0,0,1,0,0,0]+[1,0,1,1,0,0,0,1])\n",
    "\t\telif label == 7:\n",
    "\t\t\thard_label = torch.tensor([0,0,0,0,0,0,0,1,0,0]+[0,1,0,0,1,0,1,0])\n",
    "\t\telif label == 8:\n",
    "\t\t\thard_label = torch.tensor([0,0,0,0,0,0,0,0,1,0]+[1,0,1,1,0,0,1,1])\n",
    "\t\telif label == 9:\n",
    "\t\t\thard_label = torch.tensor([0,0,0,0,0,0,0,0,0,1]+[1,0,1,1,0,0,1,0])\n",
    "\n",
    "\t\t# pdb.set_trace()\n",
    "\n",
    "\t\t# hard_label = torch.zeros((self.num_classes,))\n",
    "\t\t# hard_label[label] = 1\n",
    "\t\thard_label = hard_label.float().to(self.device)\n",
    "\t\treturn hard_label\n",
    "\n",
    "\n",
    "def load_mnist_dataloader(split,bsz):\n",
    "\tdataset = MNISTDatasetWithConcepts(split = split, num_classes = 10, transform=ToTensor())\n",
    "\tdataloader = DataLoader(dataset, batch_size=bsz, shuffle=True)\n",
    "\treturn dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = load_mnist_dataloader(split = \"train\",bsz=64)\n",
    "test_loader = load_mnist_dataloader(split = \"test\",bsz=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "for images, labels, concepts in train_loader:\n",
    "    img_idx = 7\n",
    "    image = images[img_idx]\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image.squeeze().cpu(), cmap=\"gray\")\n",
    "    plt.title(f\"Label: {labels[img_idx].argmax()}, Concepts: {concepts[img_idx]}\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class g(nn.Module):\n",
    "    \"\"\"The network g consists of 2 convolutional\n",
    "    layers with 32 channels each, along with a maxpool\n",
    "    layer in between followed by a fully connected\n",
    "    layer.\"\"\"\n",
    "\n",
    "    def __init__(self, n_concepts):\n",
    "        super(g, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
    "        self.fc1 = nn.Linear(800, 128)\n",
    "        self.fc2 = nn.Linear(128, 10+n_concepts*2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        non_overlapping, overlapping = x[:,:10], x[:,10:]\n",
    "        return non_overlapping, overlapping\n",
    "\n",
    "class f(nn.Module):\n",
    "    def __init__(self, input_size) -> None:\n",
    "        super(f, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def split_concepts(concepts):\n",
    "    return concepts[:,:10], concepts[:,10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Module):\n",
    "    def __init__(self, n_concepts):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.g_model = g(n_concepts).to(device)\n",
    "        self.f_model = f(10+n_concepts*2).to(device)\n",
    "\n",
    "        # Defining the training parameters for the concepts model g\n",
    "        self.g_optimizer = torch.optim.Adam(self.g_model.parameters(), lr=1e-4)\n",
    "        self.g_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Defining the training parameters for the prediction model f\n",
    "        self.learned_g = False\n",
    "        self.f_optimizer = torch.optim.Adam(self.f_model.parameters(), lr=1e-4)\n",
    "        self.f_criterion = nn.CrossEntropyLoss()\n",
    "        self.name = 'sequential'\n",
    "\n",
    "    def split_concepts(self, concepts):\n",
    "        return concepts[:,:10], concepts[:,10:]\n",
    "\n",
    "    def loss_overlapping(self, overlapping, y_true):\n",
    "        total_loss = 0\n",
    "        for i in range(0, overlapping.shape[1], 2):\n",
    "            loss = self.g_criterion(overlapping[:, i:i+2], y_true[:, int(i/2)].long())\n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def train_g(self, train_loader, epochs):\n",
    "        self.g_model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for images, labels, concepts in train_loader:\n",
    "                images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "                y_true_non_overlapping, y_true_overlapping = self.split_concepts(concepts)\n",
    "\n",
    "                self.g_optimizer.zero_grad()\n",
    "                non_overlapping, overlapping = self.g_model(images)\n",
    "\n",
    "                # Calculate the loss for non_overlaaping concepts\n",
    "                loss_non_overlapping = self.g_criterion(non_overlapping, y_true_non_overlapping.argmax(dim=1))\n",
    "\n",
    "                # Calculate the loss for overlapping concepts\n",
    "                loss_overlapping = self.loss_overlapping(overlapping, y_true_overlapping)\n",
    "                \n",
    "                # Calculate the total loss\n",
    "                loss = loss_non_overlapping + loss_overlapping\n",
    "                loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "            print(f\"Epoch: {epoch+1}, Loss: {loss.item():.3f}\")\n",
    "        self.learned_g = True\n",
    "\n",
    "    def calc_acc_non_overlapping(self, test_loader):\n",
    "        self.g_model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, _, concepts in test_loader:\n",
    "                images, concepts = images.to(device), concepts.to(device)\n",
    "                y_true_non_overlapping, _ = self.split_concepts(concepts)\n",
    "                non_overlapping, _ = self.g_model(images)\n",
    "                y_pred_non_overlapping = non_overlapping.argmax(dim=1)\n",
    "                correct += (y_pred_non_overlapping == y_true_non_overlapping.argmax(dim=1)).sum().item()\n",
    "                total += y_true_non_overlapping.size(0)\n",
    "\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "    def calc_acc_overlapping(self, test_loader):\n",
    "        self.g_model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, _, concepts in test_loader:\n",
    "                images, concepts = images.to(device), concepts.to(device)\n",
    "                _, y_true_overlapping = self.split_concepts(concepts)\n",
    "                _, overlapping = self.g_model(images)\n",
    "\n",
    "                for i in range(0, overlapping.shape[1], 2):\n",
    "                    y_pred_overlapping = overlapping[:, i:i+2].argmax(dim=1)\n",
    "                    correct += (y_pred_overlapping == y_true_overlapping[:, int(i/2)]).sum().item()\n",
    "                    total += y_true_overlapping.size(0)\n",
    "\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "    def calc_acc_g(self, test_loader):\n",
    "        self.g_model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, _, concepts in test_loader:\n",
    "                images, concepts = images.to(device), concepts.to(device)\n",
    "                y_true_non_overlapping, y_true_overlapping = self.split_concepts(concepts)\n",
    "                non_overlapping, overlapping = self.g_model(images)\n",
    "\n",
    "                y_pred_non_overlapping = non_overlapping.argmax(dim=1)\n",
    "                correct += (y_pred_non_overlapping == y_true_non_overlapping.argmax(dim=1)).sum().item()\n",
    "                total += y_true_non_overlapping.size(0)\n",
    "\n",
    "                for i in range(0, overlapping.shape[1], 2):\n",
    "                    y_pred_overlapping = overlapping[:, i:i+2].argmax(dim=1)\n",
    "                    correct += (y_pred_overlapping == y_true_overlapping[:, int(i/2)]).sum().item()\n",
    "                    total += y_true_overlapping.size(0)\n",
    "\n",
    "        return correct / total if total > 0 else 0\n",
    "    \n",
    "    def train_f(self, train_loader, epochs):\n",
    "        if not self.learned_g:\n",
    "            raise \"You have to train g before training f in sequential training\"\n",
    "        self.f_model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for images, labels, _ in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                self.f_optimizer.zero_grad()\n",
    "\n",
    "                non_overlapping, overlapping = self.g_model(images)\n",
    "                # Concatenate the concepts\n",
    "                overlapping_concat = torch.cat([non_overlapping, overlapping], dim=1)\n",
    "                y_pred = self.f_model(overlapping_concat)\n",
    "                loss = self.f_criterion(y_pred, labels.argmax(dim=1))\n",
    "                loss.backward()\n",
    "                self.f_optimizer.step()\n",
    "            print(f\"Epoch: {epoch+1}, Loss: {loss.item():.3f}\")\n",
    "\n",
    "    def calc_acc_prediction(self, test_loader, delta=None):\n",
    "        accuracies = []\n",
    "        for images, labels, _ in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            if delta is not None:\n",
    "                y_pred = self.forward(images + delta[:images.shape[0]]).argmax(dim=1)\n",
    "            else:\n",
    "                y_pred = self.forward(images).argmax(dim=1)\n",
    "            acc = (y_pred == labels.argmax(dim=1)).float().mean().item()\n",
    "            accuracies.append(acc)\n",
    "        return sum(accuracies)/len(accuracies)\n",
    "\n",
    "    def train(self, train_loader, epochs=20):\n",
    "        self.train_g(train_loader, epochs)\n",
    "        self.train_f(train_loader, epochs)\n",
    "    def forward(self, x):\n",
    "        non_overlapping, overlapping = self.g_model(x)\n",
    "        # Concatenate the concepts\n",
    "        overlapping = torch.cat([non_overlapping, overlapping], dim=1)\n",
    "        return self.f_model(overlapping)\n",
    "\n",
    "    def save_g(self):\n",
    "        torch.save(self.g_model.state_dict(), \"g_model1.pth\")\n",
    "\n",
    "    def load_g(self, path):\n",
    "        self.g_model.load_state_dict(torch.load(path))\n",
    "        self.learned_g = True\n",
    "\n",
    "    def save_f(self):\n",
    "        torch.save(self.f_model.state_dict(), \"f_model1.pth\")\n",
    "\n",
    "    def load_f(self, path):\n",
    "        self.f_model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.725\n",
      "Epoch: 2, Loss: 1.329\n",
      "Epoch: 3, Loss: 1.445\n",
      "Epoch: 4, Loss: 0.605\n",
      "Epoch: 5, Loss: 0.171\n",
      "Epoch: 6, Loss: 1.510\n",
      "Epoch: 7, Loss: 0.449\n",
      "Epoch: 8, Loss: 0.476\n",
      "Epoch: 9, Loss: 0.129\n",
      "Epoch: 10, Loss: 0.289\n",
      "Epoch: 11, Loss: 0.668\n",
      "Epoch: 12, Loss: 0.155\n",
      "Epoch: 13, Loss: 0.200\n",
      "Epoch: 14, Loss: 0.129\n",
      "Epoch: 15, Loss: 0.076\n",
      "Epoch: 16, Loss: 0.062\n",
      "Epoch: 17, Loss: 0.016\n",
      "Epoch: 18, Loss: 0.350\n",
      "Epoch: 19, Loss: 0.152\n",
      "Epoch: 20, Loss: 0.049\n",
      "Epoch: 1, Loss: 0.254\n",
      "Epoch: 2, Loss: 0.096\n",
      "Epoch: 3, Loss: 0.062\n",
      "Epoch: 4, Loss: 0.061\n",
      "Epoch: 5, Loss: 0.004\n",
      "Epoch: 6, Loss: 0.003\n",
      "Epoch: 7, Loss: 0.002\n",
      "Epoch: 8, Loss: 0.123\n",
      "Epoch: 9, Loss: 0.002\n",
      "Epoch: 10, Loss: 0.011\n",
      "Epoch: 11, Loss: 0.252\n",
      "Epoch: 12, Loss: 0.042\n",
      "Epoch: 13, Loss: 0.014\n",
      "Epoch: 14, Loss: 0.076\n",
      "Epoch: 15, Loss: 0.006\n",
      "Epoch: 16, Loss: 0.019\n",
      "Epoch: 17, Loss: 0.001\n",
      "Epoch: 18, Loss: 0.000\n",
      "Epoch: 19, Loss: 0.020\n",
      "Epoch: 20, Loss: 0.028\n"
     ]
    }
   ],
   "source": [
    "n_concepts = 8\n",
    "sequential = Sequential(8)\n",
    "sequential.train(train_loader, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the joint_model\n",
    "torch.save(sequential.state_dict(), \"./models/sequential model/sequential3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9892515923566879"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing sequential model\n",
    "\n",
    "sequential = Sequential(8)\n",
    "sequential.load_state_dict(torch.load(\"./models/sequential model/sequential3.pth\"))\n",
    "sequential.calc_acc_prediction(test_loader  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 1\n",
      "Running iteration 2\n",
      "Running iteration 3\n",
      "Running iteration 4\n",
      "Running iteration 5\n",
      "Running iteration 6\n",
      "Running iteration 7\n",
      "Running iteration 8\n",
      "Running iteration 9\n",
      "Running iteration 10\n",
      "Running iteration 11\n",
      "Running iteration 12\n",
      "Running iteration 13\n",
      "Running iteration 14\n",
      "Running iteration 15\n",
      "Running iteration 16\n",
      "Running iteration 17\n",
      "Running iteration 18\n",
      "Running iteration 19\n"
     ]
    }
   ],
   "source": [
    "def pgd_linf_targ(model, data_loader, epsilon, alpha, num_iter, y_targ, example):\n",
    "    \"\"\"\n",
    "      Training on the whole test set\n",
    "    \"\"\"\n",
    "    delta = torch.zeros_like(example, requires_grad=True)\n",
    "    for t in range(num_iter):\n",
    "      print(f\"Running iteration {t}\")\n",
    "      for X, y, concepts in data_loader:\n",
    "        X, y, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "        yp = model(X + delta)\n",
    "        loss = 2*yp[:, y_targ].sum() - yp.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        delta = delta + alpha * delta.grad.detach().sign()\n",
    "        delta = delta.clamp(-epsilon, epsilon).detach().requires_grad_(True)\n",
    "\n",
    "        # Clear gradients after updating delta\n",
    "        if delta.grad is not None:\n",
    "            delta.grad.zero_()\n",
    "    return delta.detach()\n",
    "\n",
    "\n",
    "for images, labels, concepts in test_loader:\n",
    "    images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "    break\n",
    "delta = pgd_linf_targ(sequential, test_loader, epsilon=0.4, alpha=1e-2, num_iter=20, y_targ=2, example=images)\n",
    "yp = sequential(images + delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09106289808917198"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential.calc_acc_prediction(test_loader, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_non_overlapping = sequential.calc_acc_non_overlapping(test_loader)\n",
    "acc_non_overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99215"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_overlapping = sequential.calc_acc_overlapping(test_loader)\n",
    "acc_overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9917333333333334"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_g = sequential.calc_acc_g(test_loader)\n",
    "acc_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9917333333333334"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Joint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Joint, self).__init__()\n",
    "        self.g_model = g(n_concepts).to(device)\n",
    "        self.f_model = f(10+n_concepts*2).to(device)\n",
    "\n",
    "        self.name = 'joint'\n",
    "\n",
    "    def forward(self, x):\n",
    "        non_overlapping, overlapping = self.g_model(x)\n",
    "        overlapping_concat = torch.cat([non_overlapping, overlapping], dim=1)\n",
    "        y_pred = self.f_model(overlapping_concat)\n",
    "        return y_pred, non_overlapping, overlapping\n",
    "    \n",
    "\n",
    "    def loss_overlapping(self, overlapping, y_true):\n",
    "        total_loss = 0\n",
    "        concept_idx = 0\n",
    "        # print(overlapping.shape)\n",
    "        for i in range(0, overlapping.shape[1], 2):\n",
    "            loss = nn.CrossEntropyLoss()(overlapping[:, i:i+2], y_true[:, concept_idx].long())\n",
    "            concept_idx += 1\n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_loss_weight, concepts_loss_weight = 0.5, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.246\n",
      "Epoch: 2, Loss: 0.472\n",
      "Epoch: 3, Loss: 0.715\n",
      "Epoch: 4, Loss: 0.858\n",
      "Epoch: 5, Loss: 0.672\n",
      "Epoch: 6, Loss: 0.188\n",
      "Epoch: 7, Loss: 0.665\n",
      "Epoch: 8, Loss: 0.118\n",
      "Epoch: 9, Loss: 0.241\n",
      "Epoch: 10, Loss: 0.076\n",
      "Epoch: 11, Loss: 0.232\n",
      "Epoch: 12, Loss: 0.317\n",
      "Epoch: 13, Loss: 0.242\n",
      "Epoch: 14, Loss: 0.071\n",
      "Epoch: 15, Loss: 0.524\n",
      "Epoch: 16, Loss: 0.078\n",
      "Epoch: 17, Loss: 0.031\n",
      "Epoch: 18, Loss: 0.063\n",
      "Epoch: 19, Loss: 0.078\n",
      "Epoch: 20, Loss: 0.037\n"
     ]
    }
   ],
   "source": [
    "def calc_loss(model, y_pred, labels, non_overlapping, overlapping, y_true_non_overlapping, y_true_overlapping, criterion, pred_loss_weight, concepts_loss_weight):\n",
    "    \n",
    "    # Calculate the loss for non_overlaaping concepts\n",
    "    loss_non_overlapping = criterion(non_overlapping, y_true_non_overlapping.argmax(dim=1))\n",
    "    # Calculate the loss for overlapping concepts\n",
    "    # print(overlapping.shape, y_true_overlapping.shape)\n",
    "    loss_overlapping = model.loss_overlapping(overlapping, y_true_overlapping)\n",
    "    # Calculate the loss for concepts\n",
    "    concept_loss = loss_non_overlapping + loss_overlapping\n",
    "\n",
    "    # Calculate the loss for prediction\n",
    "    pred_loss = criterion(y_pred, labels.argmax(dim=1))\n",
    "\n",
    "    loss = pred_loss_weight * pred_loss + concepts_loss_weight * concept_loss\n",
    "    return loss\n",
    "\n",
    "def train_joint(model, train_loader, epochs, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels, concepts in train_loader:\n",
    "            images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_true_non_overlapping, y_true_overlapping = split_concepts(concepts)\n",
    "            y_pred, non_overlapping, overlapping = model(images)\n",
    "\n",
    "            loss = calc_loss(model, y_pred, labels, non_overlapping, overlapping, y_true_non_overlapping, y_true_overlapping, criterion, pred_loss_weight, concepts_loss_weight )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {loss.item():.3f}\")\n",
    "\n",
    "joint_model = Joint().to(device)\n",
    "optimizer = torch.optim.Adam(joint_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "train_joint(joint_model, train_loader, epochs, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the joint_model\n",
    "torch.save(joint_model.state_dict(), \"./models/joint/joint_model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.988953025477707"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_joint(model, test_loader):\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    for images, labels, concepts in test_loader:\n",
    "        images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "        y_pred, _, _ = model(images)\n",
    "        acc = (y_pred.argmax(dim=1) == labels.argmax(dim=1)).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "    return sum(accuracies)/len(accuracies)\n",
    "test_joint(joint_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 0\n",
      "Running iteration 1\n",
      "Running iteration 2\n",
      "Running iteration 3\n",
      "Running iteration 4\n",
      "Running iteration 5\n",
      "Running iteration 6\n",
      "Running iteration 7\n",
      "Running iteration 8\n",
      "Running iteration 9\n",
      "Running iteration 10\n",
      "Running iteration 11\n",
      "Running iteration 12\n",
      "Running iteration 13\n",
      "Running iteration 14\n",
      "Running iteration 15\n",
      "Running iteration 16\n",
      "Running iteration 17\n",
      "Running iteration 18\n",
      "Running iteration 19\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     26\u001b[0m delta \u001b[38;5;241m=\u001b[39m pgd_linf_targ(joint_model, test_loader, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, num_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, y_targ\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, example\u001b[38;5;241m=\u001b[39mimages)\n\u001b[1;32m---> 27\u001b[0m yp, _,_,_ \u001b[38;5;241m=\u001b[39m joint_model(images \u001b[38;5;241m+\u001b[39m delta)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "def pgd_linf_targ(model, data_loader, epsilon, alpha, num_iter, y_targ, example):\n",
    "    \"\"\"\n",
    "      Training on the whole test set\n",
    "    \"\"\"\n",
    "    delta = torch.zeros_like(example, requires_grad=True)\n",
    "    for t in range(num_iter):\n",
    "      print(f\"Running iteration {t}\")\n",
    "      for X, y, concepts in data_loader:\n",
    "        X, y, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "        yp, _, _ = model(X + delta)\n",
    "        loss = 2*yp[:, y_targ].sum() - yp.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        delta = delta + alpha * delta.grad.detach().sign()\n",
    "        delta = delta.clamp(-epsilon, epsilon).detach().requires_grad_(True)\n",
    "\n",
    "        # Clear gradients after updating delta\n",
    "        if delta.grad is not None:\n",
    "            delta.grad.zero_()\n",
    "    return delta.detach()\n",
    "\n",
    "\n",
    "for images, labels, concepts in test_loader:\n",
    "    images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "    break\n",
    "delta = pgd_linf_targ(joint_model, test_loader, epsilon=0.4, alpha=1e-2, num_iter=20, y_targ=2, example=images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp, _,_ = joint_model(images + delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17844347133757962"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_joint_addv(model, test_loader, delta):\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    for images, labels, concepts in test_loader:\n",
    "        images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "        y_pred, _, _ = model(images + delta[:images.shape[0]])\n",
    "        acc = (y_pred.argmax(dim=1) == labels.argmax(dim=1)).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "    return sum(accuracies)/len(accuracies)\n",
    "test_joint_addv(joint_model, test_loader, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(CNN, self).__init__()\n",
    "      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "      self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
    "      self.fc1 = nn.Linear(800, 128)\n",
    "      self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "      self.name = 'CNN'\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, 2)\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, 2)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=1e-4)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtrain_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[108], line 4\u001b[0m, in \u001b[0;36mtrain_cnn\u001b[1;34m(model, train_loader, epochs, optimizer, criterion)\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels, _ \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      5\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\Program Files\\miniconda\\envs\\CBM_thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Program Files\\miniconda\\envs\\CBM_thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Program Files\\miniconda\\envs\\CBM_thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Program Files\\miniconda\\envs\\CBM_thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m, in \u001b[0;36mMNISTDatasetWithConcepts.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     34\u001b[0m onehot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes,))\n\u001b[0;32m     35\u001b[0m onehot[label] \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 36\u001b[0m concept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_concepts_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m label \u001b[38;5;241m=\u001b[39m onehot\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),label,concept]\n",
      "Cell \u001b[1;32mIn[2], line 66\u001b[0m, in \u001b[0;36mMNISTDatasetWithConcepts.make_concepts_mnist\u001b[1;34m(self, label)\u001b[0m\n\u001b[0;32m     60\u001b[0m \thard_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# hard_label = torch.zeros((self.num_classes,))\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# hard_label[label] = 1\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m hard_label \u001b[38;5;241m=\u001b[39m \u001b[43mhard_label\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hard_label\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_cnn(model, train_loader, epochs, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels, _ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(images)\n",
    "            loss = criterion(y_pred, labels.argmax(dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {loss.item():.3f}\")\n",
    "\n",
    "train_cnn(cnn_model, train_loader, epochs, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_linf_targ(model, data_loader, epsilon, alpha, num_iter, y_targ, example):\n",
    "    \"\"\"\n",
    "      Training on the whole test set\n",
    "    \"\"\"\n",
    "    delta = torch.zeros_like(example, requires_grad=True)\n",
    "    for t in range(num_iter):\n",
    "      for X, y, concepts in data_loader:\n",
    "        X, y, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "        if model.name == 'joint':\n",
    "          yp, _, _ = model(X + delta)\n",
    "        else:\n",
    "          yp = model(X + delta)\n",
    "        loss = 2*yp[:, y_targ].sum() - yp.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        delta = delta + alpha * delta.grad.detach().sign()\n",
    "        delta = delta.clamp(-epsilon, epsilon).detach().requires_grad_(True)\n",
    "\n",
    "        # Clear gradients after updating delta\n",
    "        if delta.grad is not None:\n",
    "            delta.grad.zero_()\n",
    "    return delta.detach()\n",
    "\n",
    "def test_joint_addv(model, test_loader, delta):\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    for images, labels, concepts in test_loader:\n",
    "        images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "        if model.name == 'joint':\n",
    "          y_pred, _, _ = model(images + delta[:images.shape[0]])\n",
    "        else:\n",
    "          y_pred = model(images + delta[:images.shape[0]])\n",
    "        acc = (y_pred.argmax(dim=1) == labels.argmax(dim=1)).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "    return sum(accuracies)/len(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epsilon': 0.05, 'alpha': 0.001, 'num_iter': 5, 'model': 'sequential', 'delta': tensor([[[[-0.0220, -0.0500, -0.0500,  ..., -0.0440,  0.0000,  0.0000],\n",
      "          [ 0.0090, -0.0430, -0.0490,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [-0.0190, -0.0350, -0.0350,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [-0.0180,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0500, -0.0370, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0190, -0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [-0.0500, -0.0500, -0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [-0.0500, -0.0500, -0.0500,  ..., -0.0450,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0500,  0.0500,  0.0360,  ...,  0.0280,  0.0000,  0.0000],\n",
      "          [ 0.0330,  0.0500,  0.0500,  ...,  0.0040,  0.0000,  0.0000],\n",
      "          [ 0.0270, -0.0500,  0.0110,  ..., -0.0020,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0100,  0.0500, -0.0100,  ..., -0.0260,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0110, -0.0190, -0.0090,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [-0.0390, -0.0130, -0.0030,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [-0.0290,  0.0250,  0.0090,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0500,  0.0500,  0.0500,  ..., -0.0180,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0270,  0.0500, -0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [-0.0490,  0.0370, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500, -0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0010,  0.0500,  0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [-0.0500, -0.0500, -0.0500,  ..., -0.0300,  0.0000,  0.0000],\n",
      "          [-0.0490,  0.0430, -0.0150,  ..., -0.0220,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0500,  0.0500,  0.0500,  ..., -0.0100,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
      "       device='cuda:0'), 'accuracy': 0.08927149681528662}\n",
      "{'epsilon': 0.05, 'alpha': 0.001, 'num_iter': 5, 'model': 'joint', 'delta': tensor([[[[ 4.9000e-02,  5.0000e-02,  4.9000e-02,  ...,  2.0000e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 4.1000e-02,  5.0000e-02,  5.0000e-02,  ..., -2.0000e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  1.0000e-03,  5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 5.0000e-02,  5.0000e-02,  4.9000e-02,  ...,  1.2000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0000e-02, -5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 5.0000e-02, -5.0000e-02,  5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0000e-02,  2.5000e-02, -5.0000e-02,  ...,  1.2000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 4.1000e-02,  3.7000e-02,  5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-5.0000e-02,  3.3000e-02, -3.9000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-4.0000e-03, -5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.0000e-02, -5.0000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -4.8000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-1.4000e-02,  6.9849e-10, -5.0000e-02,  ...,  3.6000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.0000e-02, -5.0000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -4.6000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.0000e-02, -5.0000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.0000e-02, -5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.0000e-02,  3.8000e-02,  2.0000e-02,  ...,  2.2000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]], device='cuda:0'), 'accuracy': 0.9856687898089171}\n",
      "{'epsilon': 0.05, 'alpha': 0.001, 'num_iter': 5, 'model': 'CNN', 'delta': tensor([[[[ 0.0500,  0.0410,  0.0500,  ..., -0.0090,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0410,  0.0500,  ..., -0.0290,  0.0000,  0.0000],\n",
      "          [-0.0260,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0410, -0.0240, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0500, -0.0500, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [-0.0500, -0.0500,  0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ..., -0.0420,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0410,  0.0060, -0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0500, -0.0500, -0.0500,  ...,  0.0280,  0.0000,  0.0000],\n",
      "          [-0.0500, -0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0240,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0060, -0.0420, -0.0240,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0500,  0.0500,  0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [-0.0360,  0.0500,  0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [-0.0500, -0.0500, -0.0500,  ..., -0.0320,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0470,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [-0.0140,  0.0420, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0500, -0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [-0.0390,  0.0500,  0.0500,  ..., -0.0160,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0120,  0.0120, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
      "       device='cuda:0'), 'accuracy': 0.9855692675159236}\n",
      "{'epsilon': 0.05, 'alpha': 0.001, 'num_iter': 10, 'model': 'sequential', 'delta': tensor([[[[-2.3000e-02, -5.0000e-02, -5.0000e-02,  ..., -4.3000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 1.0000e-02, -4.2000e-02, -5.0000e-02,  ..., -4.9000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-2.0000e-02, -3.6000e-02, -3.6000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-1.9000e-02,  5.0000e-02,  5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.0000e-02, -3.6000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 4.9000e-02,  2.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-5.0000e-02, -5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.0000e-02, -5.0000e-02, -5.0000e-02,  ..., -4.4000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0000e-02,  5.0000e-02,  3.5000e-02,  ...,  2.7000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 3.2000e-02,  5.0000e-02,  5.0000e-02,  ...,  3.0000e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 2.8000e-02, -5.0000e-02,  1.0000e-02,  ..., -3.0000e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 9.0000e-03,  5.0000e-02, -9.0000e-03,  ..., -2.5000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0000e-02, -1.8000e-02, -8.0000e-03,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-3.8000e-02, -1.2000e-02, -2.0000e-03,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-2.8000e-02,  2.6000e-02,  1.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -1.9000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.6000e-02,  5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-4.8000e-02,  3.8000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.6310e-08,  5.0000e-02,  5.0000e-02,  ..., -4.9000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-5.0000e-02, -5.0000e-02, -5.0000e-02,  ..., -2.9000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-5.0000e-02,  4.2000e-02, -1.6000e-02,  ..., -2.3000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -9.0000e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]], device='cuda:0'), 'accuracy': 0.0878781847133758}\n",
      "{'epsilon': 0.05, 'alpha': 0.001, 'num_iter': 10, 'model': 'joint', 'delta': tensor([[[[ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ...,  1.0000e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 4.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -3.0000e-03,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02, -3.4925e-09,  5.0000e-02,  ..., -4.9000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ...,  1.1000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0000e-02, -5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 5.0000e-02, -5.0000e-02,  5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0000e-02,  2.6000e-02, -5.0000e-02,  ...,  1.3000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 4.2000e-02,  3.8000e-02,  5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-5.0000e-02,  3.4000e-02, -3.8000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-3.0000e-03, -5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.0000e-02, -5.0000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -4.9000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-1.3000e-02, -1.0000e-03, -5.0000e-02,  ...,  3.7000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.0000e-02, -5.0000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -4.7000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.0000e-02, -5.0000e-02, -5.0000e-02,  ..., -5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.0000e-02, -5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02, -5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 5.0000e-02,  5.0000e-02,  5.0000e-02,  ...,  5.0000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.0000e-02,  3.9000e-02,  2.1000e-02,  ...,  2.1000e-02,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]], device='cuda:0'), 'accuracy': 0.9858678343949044}\n",
      "{'epsilon': 0.05, 'alpha': 0.001, 'num_iter': 10, 'model': 'CNN', 'delta': tensor([[[[ 0.0500,  0.0420,  0.0500,  ...,  0.0100,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0400,  0.0500,  ..., -0.0200,  0.0000,  0.0000],\n",
      "          [-0.0270,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0400, -0.0250, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0500, -0.0500, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [-0.0500, -0.0500,  0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ..., -0.0410,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0420,  0.0050, -0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0500, -0.0500, -0.0500,  ...,  0.0270,  0.0000,  0.0000],\n",
      "          [-0.0500, -0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0230,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0070, -0.0430, -0.0250,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0500,  0.0500,  0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [-0.0370,  0.0500,  0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0490,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [-0.0500, -0.0500, -0.0500,  ..., -0.0310,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0460,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [-0.0150,  0.0410, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0500, -0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          [-0.0400,  0.0500,  0.0500,  ..., -0.0150,  0.0000,  0.0000],\n",
      "          [ 0.0500,  0.0500,  0.0500,  ...,  0.0500,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0130,  0.0110, -0.0500,  ..., -0.0500,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
      "       device='cuda:0'), 'accuracy': 0.9857683121019108}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Attack\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[43mpgd_linf_targ\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_targ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m experiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m delta\n\u001b[0;32m     38\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m test_joint_addv(model, test_loader, delta)\n",
      "Cell \u001b[1;32mIn[113], line 7\u001b[0m, in \u001b[0;36mpgd_linf_targ\u001b[1;34m(model, data_loader, epsilon, alpha, num_iter, y_targ, example)\u001b[0m\n\u001b[0;32m      5\u001b[0m delta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(example, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iter):\n\u001b[1;32m----> 7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m X, y, concepts \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m      8\u001b[0m     X, y, concepts \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device), concepts\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoint\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32md:\\Program Files\\miniconda\\envs\\CBM_thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Program Files\\miniconda\\envs\\CBM_thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Program Files\\miniconda\\envs\\CBM_thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Program Files\\miniconda\\envs\\CBM_thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m, in \u001b[0;36mMNISTDatasetWithConcepts.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     34\u001b[0m onehot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes,))\n\u001b[0;32m     35\u001b[0m onehot[label] \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 36\u001b[0m concept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_concepts_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m label \u001b[38;5;241m=\u001b[39m onehot\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),label,concept]\n",
      "Cell \u001b[1;32mIn[2], line 66\u001b[0m, in \u001b[0;36mMNISTDatasetWithConcepts.make_concepts_mnist\u001b[1;34m(self, label)\u001b[0m\n\u001b[0;32m     60\u001b[0m \thard_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# hard_label = torch.zeros((self.num_classes,))\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# hard_label[label] = 1\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m hard_label \u001b[38;5;241m=\u001b[39m \u001b[43mhard_label\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hard_label\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilons = [0.05, 0.1, 0.2, 0.3]\n",
    "alphas = [1e-3, 1e-2, 0.1]\n",
    "num_iterations = [5, 10, 20]\n",
    "# epsilons = [0.05]\n",
    "# alphas = [1e-3]\n",
    "# num_iterations = [5]\n",
    "\n",
    "sequential = Sequential(8)\n",
    "sequential.load_state_dict(torch.load(\"models\\sequential model\\sequential3.pth\"))\n",
    "\n",
    "joint_model = Joint().to(device)\n",
    "joint_model.load_state_dict(torch.load(\"models\\joint\\joint_model2.pth\"))\n",
    "\n",
    "cnn_model = CNN().to(device)\n",
    "cnn_model.load_state_dict(torch.load(\"models\\cnn1.pth\"))\n",
    "models = [sequential, joint_model, cnn_model]\n",
    "\n",
    "experiments = []\n",
    "\n",
    "for images, labels, concepts in test_loader:\n",
    "    images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "    break\n",
    "\n",
    "\n",
    "for epsilon in epsilons:\n",
    "  for alpha in alphas:\n",
    "    for num_iter in num_iterations:\n",
    "      experiment = {\n",
    "          'epsilon': epsilon,\n",
    "          'alpha': alpha,\n",
    "          'num_iter': num_iter\n",
    "      }\n",
    "      for model in models:\n",
    "        experiment['model'] = model.name\n",
    "        # Attack\n",
    "        delta = pgd_linf_targ(model, test_loader, epsilon, alpha, num_iter, y_targ=2, example=images)\n",
    "        experiment['delta'] = delta\n",
    "        accuracy = test_joint_addv(model, test_loader, delta)\n",
    "        experiment['accuracy'] = accuracy\n",
    "        print(experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBM_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
